\section{Satellite galaxies around a massive central - part 2}

Because I could copy all of the functions from the working classes, 
my code for those is similar to my sister\'s, Liz van der Kamp (s2135752). 
For this exercise I needed to use different minimizing functions, LU decomposition for Levenberg-Marquardt
and an integrating function from a previous handin/working class.
I used pieces of code from handin2 excercise 1, like my n(x) function and my Romberg integration code.

The code I wrote for this is:
\lstinputlisting{NUR_handin3Q1.py}

\subsection*{a}

For this part I used a Golden Section Search algorithm on -N(x) = -$4\pi *x^2$n(x) to find the maximum of N(x),
because Golden Section search finds the minimum so we find the minimum of the flipped function. I use Golden Section
Search because it is a simple 1 dimensional algorithm which should find the minimum/maximum accurately yet quickly.
Before choosing an initial bracket to search, I plot the function to inspect it. I see that the maximum is around 
x~0.5 so as initial bracket I do [0,0.5,5] with a maximum of 100 iterations and an accuracy of 10$^{-15]$. 
With this I find a maximum at x~0.23, the exact value is
\lstinputlisting{NUR3Q1maxim.txt}

To make sure I found the maximum, I plotted the found maximum and the function in Fig \ref{fig1}.
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\linewidth]{NUR3Q1plot1.pdf}
  \caption{Plot corresponding to exercise 1a, showing  N(x) and the maximum found by Golden section search.}
  \label{fig:fig1}
\end{figure} 


\subsection*{b}

For this part I used the example python code to read in files. Then, in order to choose bins, I inspect the data
The range of the function is 0 until 5, so I want real spaced bins since we start at 0 and the data already looks
Poissonian in real space. Furthermore, the biggest amount of data points we have is around 20 000 000, which is about 
5 * $10^{-8}$ inverted. N(x) reaches this value at about x~2, so at this point we expect a bin to have at most 1 count,
which is why I take bins in a range from 0 until 2. The first dataset might still have some counts above x~2, but
I didn't want the other data sets to have too many zero bins, so I take [0,2].
Then for the number of bins I look at the histograms to see when they look somewhat smooth and Poissonian, and at 
25 bins I find a good balance between enough bins for the big datasets and not too many for the datasets with less
datapoints.

For each dataset, the mean number of satellites, $\lange N_{sat} \rangle$ is the number of satellites divided by
the number of halos, aka it is the length of the dataset divided by the number of halos. 
The mean number of satellites per halo in each radial bin, N$_i$ is the number of satellites in a bin divided by
the number of halos and divided by the binwidth. 

Then I minimize a $\chi^2$ using a Levenberg-Marquardt routine and give the variance per bin by integrating N(x) 
over the x-range of the bins. I also calculate the normalization factor A given the initial guess for [a,b,c] =
[2.2, 0.5, 1.6], which I picked based on the values given in (a) and then tweaked a little to fit the data better.
A is a global variable so that I don't have to put it in the function itself and I can give the function as a variable
in the Levenberg-Marquardt function. A is recalculated every time a new guess for [a,b,c] is given, and I do so
by integrating as in handin2 question 1. 
The $\lange N_{sat} \rangle$, best fit a,b,c and minimum $\chi^2$ for each dataset are: 1)
\lstinputlisting{NUR3Q1chi21.txt}
2)
\lstinputlisting{NUR3Q1chi22.txt}
3)
\lstinputlisting{NUR3Q1chi23.txt}
4)
\lstinputlisting{NUR3Q1chi24.txt}
5)
\lstinputlisting{NUR3Q1chi25.txt}

In Fig. \ref{fig:fig2} you can see the $\chi^2$ fits found by the Levenberg-Marquardt routine. Only for the first dataset the fit is not optimal, which could be due to the initial parameters given to the routine or the fact that it found a local minimum instead of a global minimum. 

\begin{figure}[ht]
    \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotchi21.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotchi22.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotchi23.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotchi24.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotchi25.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
    \caption{loglog plots of the binned data and the $\chi^2$ fits for the 5 datasets. For the first dataset, the Levenberg-Marquardt routine had a tough time finding a good fit.}
    \label{fig:fig2}
\end{figure}


\subsection*{c}

I use the same bins as in (b) and write a function which calculates the minus log likelihood of a Poisson fit and the derivatives of that with respect to the fit parameters. In both functions I recalculate A to make sure that the normalization for each fit is (somewhat) correct. In the function I don't calculate A as accurately as for the initial and final fit value so that it doesn't take too long. 
Now, I use a Quasi Newton routine to find the minimum of the minus log likelihood with an intial lambda of $10^{-3}$. To find the optimal step size lambda, I do line minimization using Golden Section Search, but sometimes the minus log likelihood doesn't exist for some parameters, so I check for NaNs before deciding between which lambda values to look. If there are no NaNs for either 1 or -1 for lambda, I search for a lambda between -15 until 15. Otherwise, I look for smaller lambda. 
For the initial guess I looked at the $\chi^2$ fit values and thus chose [1.99,0.5,1.6] as initial guess for a,b,c and ran the Quasi Newton routine, again scaling the final fit so the total number for the data and the fit are the same.

The best fit a,b,c and minimum $-\lnL$ for each dataset are: 1)
\lstinputlisting{NUR3Q1pois1.txt}
2)
\lstinputlisting{NUR3Q1pois2.txt}
3)
\lstinputlisting{NUR3Q1pois3.txt}
4)
\lstinputlisting{NUR3Q1pois4.txt}
5)
\lstinputlisting{NUR3Q1pois5.txt}


In Fig. \ref{fig:fig3} you can see the Poisson fits found by the Quasi Newton routine. Only for the third dataset the fit is not optimal, which could be due to the initial parameters given to the routine or the fact that it found a local minimum instead of a global minimum. 

\begin{figure}[ht]
    \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotpois1.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotpois2.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotpois3.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotpois4.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
     \begin{subfigure}{.49\textwidth}
       \centering
    \includegraphics[scale=0.5]{NUR3Q1plotpois5.pdf}
    \centering
    \subcaption{}
    \label{}
    \end{subfigure}
    \caption{loglog plots of the binned data and the Poisson fits for the 5 datasets. For the third dataset, the Quasi Newton routine had a tough time finding a good fit.}
    \label{fig:fig3}
\end{figure}


\subsection*{d}

To do the G-test, I need integer counts for the observed counts, so I use the unnormalized bincounts and unnormalize the fit counts too to be able to compare them and get a sensible answer out of the G test.
Then, to calculate Q I import the upper incomplete gamma function and the gamma function from scipy. The number of degrees of freedom is the number of data points minus the fit parameters minus 1, which in my case is 25 - 3 - 1 = 21. (The minus one comes from the fact that not all parameters are independent.)
Using k=21 and the calculated G for the fit I calculate Q, and the results are:

in order we have the G for the $\chi^2$ fit, G for the poisson fit, Q for the $\chi^2$ fit, and Q for the poisson fit. Per dataset the output is: 
1)
\lstinputlisting{NUR3Q1GQ1.txt}
2)
\lstinputlisting{NUR3Q1GQ2.txt}
3)
\lstinputlisting{NUR3Q1GQ3.txt}
4)
\lstinputlisting{NUR3Q1GQ4.txt}
5)
\lstinputlisting{NUR3Q1GQ5.txt}

For the first 2 datasets the G is so big that both Q's are 1.0, even for the first $\chi^2$ fit, which by eye does not look optimal. This is most likely because the counts in the bins are so large. 
For dataset 3 and 4 the poisson fit gives a better Q value than the $\chi^2$ fit, but it is not by much. 
And for dataset 5 the $\chi^2$ Q is marginally better than the one for poisson, but they are almost equal. 

It makes sense that for the datasets with less counts the poisson is better because the more counts, the more the distribution starts to look like a gaussian instead of poissonian. For dataset 5 it is probably by chance that the $\chi^2$ fit gives a good Q value. 


